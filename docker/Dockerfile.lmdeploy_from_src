FROM nvcr.io/nvidia/tritonserver:22.12-py3

RUN rm /etc/apt/sources.list.d/cuda*.list && apt-get update && apt-get install -y --no-install-recommends \
    rapidjson-dev libgoogle-glog-dev gdb  \
    && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --no-cache-dir torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
RUN python3 -m pip install --no-cache-dir cmake packaging

ENV NCCL_LAUNCH_MODE=GROUP

RUN python3.8 -m pip install --no-cache-dir --trusted-host pypi.shopee.io -i http://pypi.shopee.io/simple/ "aip-infer-utils>=0.6.2"
WORKDIR /opt/LMDeploy
ADD . /opt/LMDeploy
RUN python3 -m pip install --no-cache-dir -r requirements.txt
ARG SM="80"
RUN mkdir -p build && cd build &&\
    cmake .. \
        -DCMAKE_BUILD_TYPE=RelWithDebInfo \
        -DCMAKE_EXPORT_COMPILE_COMMANDS=1 \
        -DCMAKE_INSTALL_PREFIX=./install \
        -DBUILD_PY_FFI=ON \
        -DBUILD_MULTI_GPU=ON \
        -DCMAKE_CUDA_FLAGS="-lineinfo" \
        -DUSE_NVTX=ON \
        -DSM=${SM} -DCMAKE_CUDA_ARCHITECTURES=${SM} && \
    make -j$(nproc) && make install && \
    cd .. && \
    python3 -m pip install . && \
    # rm -rf build
    ls build
ENV LD_LIBRARY_PATH=/opt/LMDeploy/build/install/lib:$LD_LIBRARY_PATH
RUN cp /opt/LMDeploy/scripts/gemm_config_v100.in /workspace/gemm_config.in
RUN cp /opt/LMDeploy/scripts/gen_gemm_config.sh /workspace/gen_gemm_config.sh

WORKDIR /workspace
ENV PROMETHEUS_MULTIPROC_DIR /workspace/metrics
RUN mkdir -p /workspace/metrics

CMD ["lmdeploy", "serve", "api_server", "./models/", "--server-name", "0.0.0.0", "--server-port", "80", "--session-len", "4096", "--cache-max-entry-count", "0.8", "--tp", "1"]
